{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Loads & Prepares Data**"
      ],
      "metadata": {
        "id": "wwdNNt7eSgvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgClAl1qSZl3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads data from the given file and extracts the ith column\n",
        "# Normalises the data into (0,1) range\n",
        "def read_data(file_name, i):\n",
        "    dataset = pd.read_csv(file_name).iloc[:, [i]]\n",
        "    data = np.array(dataset.values.astype('float32'))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data).flatten()\n",
        "    return data"
      ],
      "metadata": {
        "id": "1RQQc2zqSovD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits the given data into a training set and a testing set\n",
        "# Based on split ratio\n",
        "def get_train_test(data, split_percent):\n",
        "    n = len(data)\n",
        "    split = int(n * split_percent)\n",
        "    train_data = data[:split]\n",
        "    test_data = data[split:]\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "iZZ_P5D0Sqa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshapes the given sequential data into a set of target variables and sets of features\n",
        "# Based on the given time steps (number of rows in the RNN)\n",
        "def get_XY(data, time_steps):\n",
        "    y_indexes = np.arange(time_steps, len(data), time_steps)\n",
        "    y_data = data[y_indexes]\n",
        "    X_rows = len(y_data)\n",
        "    X_data = data[range(X_rows*time_steps)]\n",
        "    X_data = np.reshape(X_data, (X_rows, time_steps, 1))\n",
        "    return X_data, y_data"
      ],
      "metadata": {
        "id": "b41Ztee0Ssa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Builds, Trains & Tests your RNN**\n"
      ],
      "metadata": {
        "id": "jazI9_mTSxAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "BPoDCrfUSyM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads data for training and testing\n",
        "data = read_data('Tasla_Stock_Updated_V2.csv', 3)\n",
        "\n",
        "# Splits the data into a training set and a testing set\n",
        "train_data, test_data = get_train_test(data, 0.8)\n",
        "\n",
        "# Reshapes the data\n",
        "X_train_data, y_train_data = get_XY(train_data, 12)\n",
        "X_test_data, y_test_data = get_XY(test_data, 12)\n",
        "\n",
        "# Builds the model\n",
        "RNN_model = tf.keras.Sequential()\n",
        "RNN_model.add(tf.keras.layers.SimpleRNN(16, activation='tanh', input_shape=(12,1)))\n",
        "RNN_model.add(tf.keras.layers.Dense(1, 'tanh'))\n",
        "\n",
        "# Compiles the model\n",
        "RNN_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Trains the model\n",
        "train_history = RNN_model.fit(X_train_data, y_train_data, epochs=100, batch_size=32)\n",
        "\n",
        "# Evaluates the model on the test data\n",
        "loss = RNN_model.evaluate(X_test_data, y_test_data)\n",
        "print('Test model loss:', round(loss,4))"
      ],
      "metadata": {
        "id": "JmoZZhjpS1BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Plots Predictions vs Actual Values Graph**"
      ],
      "metadata": {
        "id": "s_Mpk97mTF6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HKE1Rut_cVhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots a predictions vs actual values graph\n",
        "def plot_lo(predictions, actual):\n",
        "  plt.plot(predictions, label='predicted value')\n",
        "  plt.plot(actual, label='actual value')\n",
        "  plt.xlabel('Observation')\n",
        "  plt.ylabel('Scaled Stock Price')\n",
        "  plt.legend(['predicted value', 'actual value'])\n",
        "  plt.grid(True)\n",
        "  plt.show"
      ],
      "metadata": {
        "id": "LiS1aZFATJWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates the predictions\n",
        "X_data = np.vstack((X_train_data, X_test_data))\n",
        "predictions = RNN_model.predict(X_data, verbose = 0)\n",
        "\n",
        "# Calculates the actual values\n",
        "y_data = np.hstack((y_train_data, y_test_data))\n",
        "\n",
        "# Finds the highest and lowest values in the data\n",
        "prediction_dataset = pd.read_csv('Tasla_Stock_Updated_V2.csv').iloc[:, [3]]\n",
        "max = np.max(prediction_dataset)\n",
        "min = np.min(prediction_dataset)\n",
        "\n",
        "# Unscales the predictions and the actual values using these values\n",
        "predictions = predictions * (max - min) + min\n",
        "actual = y_data * (max - min) + min\n",
        "\n",
        "# Plots the graph using the unscaled predictions and the unscaled actual values\n",
        "plot_lo(predictions, actual)"
      ],
      "metadata": {
        "id": "zO5VFmMITMIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Applies the Model**"
      ],
      "metadata": {
        "id": "jFZrIAB5TX62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads data for predictions\n",
        "prediction_data = read_data('Tasla_Stock_Updated_V2.csv', 3)\n",
        "X_prediction_data, y_prediction_data = get_XY(prediction_data, 12)\n",
        "\n",
        "# Calculates the predictions\n",
        "predictions = RNN_model.predict(X_prediction_data, verbose = 0)\n",
        "\n",
        "# Stores the dates of the predictions\n",
        "y_indexes = np.arange(12, len(prediction_data), 12)\n",
        "dates = pd.read_csv('Tasla_Stock_Updated_V2.csv').iloc[:, [1]]\n",
        "\n",
        "# Displays the predictions in a readable format\n",
        "j = 0\n",
        "print('Predictions:')\n",
        "for i in y_indexes:\n",
        "    output = f'{predictions[j,0]} will be the highest Tesla stock price on {dates.iloc[i, 0]}.'\n",
        "    j = j + 1\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "0UvoarmiTZKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}